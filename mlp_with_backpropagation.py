# -*- coding: utf-8 -*-
"""MLP_with_Backpropagation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zZ4mEaZAC2srin_bxqjBkNgeKJ921FKX
"""

import numpy as np
from sklearn.datasets import make_moons
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

class MLP:
  def __init__(self, input_size, hidden_size, output_size,learning_rate=0.01):
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.learning_rate = learning_rate

#Initialize weights and biases
    self.w1 = np.random.randn(self.input_size, self.hidden_size)# Weights between input and hidden layer
    self.b1 = np.zeros((1,self.hidden_size))# Biases for hidden layer

    self.w2 = np.random.randn(self.hidden_size, self.output_size)# Weights between hidden and output layer
    self.b2 = np.zeros((1,self.output_size)) # Biases for output layer

#Activation function (Sigmoid)
  def sigmoid(self,x):
    return 1/(1 + np.exp(-x))

  def sigmoid_derivative(self,x):# Derivative of sigmoid
    return x * (1 - x)
# Forward pass
  def forward(self,x):
    self.z1 = np.dot(x, self.w1) + self.b1  # Linear transformation (Input → Hidden)
    self.a1 = self.sigmoid(self.z1)  # Apply activation function (Hidden layer output)
    self.z2 = np.dot(self.a1, self.w2) + self.b2 # Linear transformation (Hidden → Output)
    self.a2 = self.sigmoid(self.z2) # Apply activation function (Output layer output)
    return self.a2 # Final prediction

  def backward(self,x,y):
    m = y.shape[0] # Number of samples

#compute error
    error = self.a2 - y # Compute error in output layer
    d_output = error * self.sigmoid_derivative(self.a2)  # Compute output gradient

#compute gradient for hidden layer
    error_hidden = np.dot(d_output, self.w2.T) # Backpropagate the error to hidden layer
    d_hidden = error_hidden * self.sigmoid_derivative(self.a1) # Compute hidden layer gradient


#update weights and biases
    self.w2 -= self.learning_rate * np.dot(self.a1.T, d_output) / m # Update weights (hidden → output)
    self.b2 -= self.learning_rate * np.sum(d_output, axis=0, keepdims=True) / m  # Update biases (output)
    self.w1 -= self.learning_rate * np.dot(x.T, d_hidden) / m  # Update weights (input → hidden)
    self.b1 -= self.learning_rate * np.sum(d_hidden, axis=0, keepdims=True) / m # Update biases (hidden)

  def train(self,x,y,epochs = 10000):
    for epoch in range(epochs):
        self.forward(x) # Perform forward pass
        self.backward(x,y) # Perform backpropagation to update weights

        if epoch % 1000 == 0:
            loss = np.mean((self.a2 - y) **2) # Compute Mean Squared Error (MSE)
            print(f"Epoch {epoch}, Loss: {loss:.6f}")  # Print loss every 1000 epochs

  def predict(self,x):
    return self.forward(x)

#Generate dataset
X, y = make_moons(n_samples=500, noise=0.2 , random_state=42)
y = y.reshape(-1,1) #Reshape for compatibility

#split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Train MLP

mlp = MLP(input_size=2, hidden_size=4, output_size=1 , learning_rate = 0.1)
mlp.train(X_train, y_train, epochs=10000)

#predictions
predictions = mlp.predict(X_test)
print("Predictions:",predictions)