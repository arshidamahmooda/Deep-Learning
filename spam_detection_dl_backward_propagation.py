# -*- coding: utf-8 -*-
"""Spam Detection DL Backward_Propagation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wIbRFEap5bqjMKsAQSNiSU0q6VLlKcMq

1. Import Necessary Libraries ->
numpy and pandas : Used for handling and processing numerical and tabular data.
train_test_split : Splits data into training and testing sets.
TfidfVectorizer : Converts text messages into numerical feature vectors using TF-IDF (Term Frequency-Inverse Document Frequency).
LabelEncoder :Encodes categorical labels (e.g., "ham" or "spam") into numerical values.
2. Load and inspect the dataset
3.  Dropping unnecessary columns (if any).
4. Encoding the labels (ham → 0, spam → 1).
5. Applying TF-IDF vectorization to transform text into numerical features.
6. Splitting the data into training and testing sets for model training.
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

# Load dataset
file_path = "/content/spam.csv"
df = pd.read_csv(file_path, encoding='latin-1')

# Inspect dataset columns
df.head()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
df = pd.read_csv("/content/spam.csv", encoding="latin-1")

# Drop unnecessary columns and rename for clarity
df = df.iloc[:, :2]
df.columns = ["Category", "Message"]

# Convert labels to binary (spam=1, ham=0)
df["Category"] = df["Category"].map({"ham": 0, "spam": 1})

# Apply TF-IDF vectorization (limit to 500 features for efficiency)
vectorizer = TfidfVectorizer(max_features=500)
X = vectorizer.fit_transform(df["Message"]).toarray()
y = df["Category"].values

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display dataset shape
X_train.shape, X_test.shape

"""1. Define Activation Functions and Loss Function
2. Initialize Neural Network Parameters  :
Input Layer -> The number of features (e.g., TF-IDF vectorized text features).
Hidden Layer -> 8 neurons (adjustable for efficiency vs. performance).
Output Layer ->1 neuron (sigmoid activation to output a probability).
Learning Rate -> Controls the step size for weight updates.
Weight Initialization ->Small random values to prevent symmetry problems.
Bias Initialization -> All zeros.
3. Train Neural Network Using Mini-Batch Gradient Descent:
Epochs -> The number of times the entire dataset is passed through the network.
Batch Size ->64 examples per iteration for efficient learning
4. Implement Forward and Backpropagation -- Shuffling and Mini-Batch Selection :Shuffling ensures randomization, improving model generalization.
Mini-batch selection processes a small subset of training data per iteration
 1. Forward Propagation
 2. Backpropagation -- Computes gradients of weights and biases using the chain rule
 3. Weight Updates--Updates weights and biases using gradient descent.
The learning rate (0.01) determines the step size.
 4. Monitor Loss Every 50 Epochs -- Every 50 epochs, we compute the loss to monitor progress.
 Lower loss = better model performance.


"""

# Define activation functions and their derivatives
def relu(Z):
    return np.maximum(0, Z)

def relu_derivative(Z):
    return (Z > 0).astype(float)

def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))

def compute_loss(y_true, y_pred):
    m = y_true.shape[0]
    loss = -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))
    return loss

# Initialize neural network parameters
input_size = X_train.shape[1]  # 500 features
hidden_size = 8                # Reduced hidden layer size for efficiency
output_size = 1                 # Binary classification (spam/ham)
learning_rate = 0.01

np.random.seed(42)
W1 = np.random.randn(input_size, hidden_size) * 0.01
b1 = np.zeros((1, hidden_size))
W2 = np.random.randn(hidden_size, output_size) * 0.01
b2 = np.zeros((1, output_size))

# Train the neural network using mini-batch gradient descent
epochs = 500
batch_size = 64

for epoch in range(epochs):
    indices = np.random.permutation(X_train.shape[0])
    X_train_shuffled, y_train_shuffled = X_train[indices], y_train[indices]

    for i in range(0, X_train.shape[0], batch_size):
        X_batch = X_train_shuffled[i:i+batch_size]
        y_batch = y_train_shuffled[i:i+batch_size].reshape(-1, 1)

        # Forward propagation
        Z1 = np.dot(X_batch, W1) + b1
        A1 = relu(Z1)
        Z2 = np.dot(A1, W2) + b2
        A2 = sigmoid(Z2)

        # Backpropagation
        dZ2 = A2 - y_batch
        dW2 = np.dot(A1.T, dZ2) / batch_size
        db2 = np.sum(dZ2, axis=0, keepdims=True) / batch_size

        dA1 = np.dot(dZ2, W2.T)
        dZ1 = dA1 * relu_derivative(Z1)
        dW1 = np.dot(X_batch.T, dZ1) / batch_size
        db1 = np.sum(dZ1, axis=0, keepdims=True) / batch_size

        # Update weights
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2

    # Compute loss for monitoring
    if epoch % 50 == 0:
        Z1_full = np.dot(X_train, W1) + b1
        A1_full = relu(Z1_full)
        Z2_full = np.dot(A1_full, W2) + b2
        A2_full = sigmoid(Z2_full)
        loss = compute_loss(y_train.reshape(-1, 1), A2_full)
        print(f"Epoch {epoch}: Loss = {loss:.4f}")

print("Training completed successfully!")

"""1. Forward Propagation on the Test Set--
 Z1_test: Computes the weighted sum of inputs (X_test) and weights (W1), adding bias (b1).
 A1_test: Applies the ReLU activation function to introduce non-linearity.
 Z2_test: Computes the weighted sum from the hidden layer output (A1_test) using weights (W2) and bias (b2).
 A2_test: Applies the sigmoid activation function to produce probabilities (values between 0 and 1) for binary classification.
2. Convert Probabilities to Binary Predictions -- This converts the probability scores into binary class labels.
If A2_test is greater than 0.5, it's classified as 1 (spam); otherwise, it's 0 (ham)
3. Compute Accuracy --accuracy_score(y_test, y_pred): Compares the predicted values (y_pred) with the actual labels (y_test).
Returns the percentage of correct predictions.
4. Compute Confusion Matrix -- confusion_matrix(y_test, y_pred): Computes a confusion matrix, which summarizes the model’s performance in terms of:
True Positives (TP): Correctly predicted spam.
True Negatives (TN): Correctly predicted ham.
False Positives (FP): Predicted spam, but it was ham.
False Negatives (FN): Predicted ham, but it was spam.
5. output --Returns the accuracy and confusion matrix as outputs.

"""

# Evaluate the model on the test set
Z1_test = np.dot(X_test, W1) + b1
A1_test = relu(Z1_test)
Z2_test = np.dot(A1_test, W2) + b2
A2_test = sigmoid(Z2_test)

# Convert probabilities to binary predictions
y_pred = (A2_test > 0.5).astype(int)

# Compute accuracy
accuracy = accuracy_score(y_test, y_pred)

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

accuracy, conf_matrix